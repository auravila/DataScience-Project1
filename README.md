# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**


This dataset contains results of marketing contact campaings from a set of bank customers also includes customer demographics, consumer & confidence indexes 
and other statistics. We seek to predict the likelihood of contracting a loan after being targeted via a marketing campaing.  


**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The approach taken to create the experiments was based on a classification model, the results of accuracy of 0.90 provides great confidence in the experiment run
nevertheless this is the result of a discrete set of values for random sampling and with a limited set of iteractions. 


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The pipeline is a sequential call to a train script which references a dataset, cleanses it, split the data into a train and test sets and calls a regresion model.
The regresion model parameter are provided by the hyperdrive model Random sampling and the discrete set of parameters defined by choice.

The metric selected to return is accuracy which is constrained by a bandit policy, i.e. if a particular run does not meet the threshold this child run is terminated.

**What are the benefits of the parameter sampler you chose?**

choosing a discrete set of parameters guarantees the same probability to each value of the list. A small set of parameters provides a quick turnaround of the
experiment results for initial evaluation.

**What are the benefits of the early stopping policy you chose?**

The policy early terminates a child run where the primary metric is not met withing the policies defined parameters.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

AutoML model is more accurate than the random sampling model by 1%. AutoML also calculated all of the additional metrics and selected the most optimized combination
of metris. This reassures that the model is accurate enough for prediction.

Regarding architecture difference, I believe is mainly in the sequential execution of a defined experiment by multiple child runs as opposed to the AutoML execution 
which runs through all of the available algorithms and retrieved the best most optimized resultset.

for this particular scenario 1% difference is inmaterial, my best guess here maybe the poor selection of hyperdrive parameters or model. AzureML makes modeling simpler
but it limits knowledgeable developers to use their own algorithms.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

Try out other types of model and algorithms and a larger sets of parameters. Generate simple predictions to confirm the validity of the model. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

Image attached to main project - root folders

Command used
delcluster = ComputeTarget.delete(MYcompute_cluster)


